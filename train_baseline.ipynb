{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d5fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # import Google Drive 套件\n",
    "# from google.colab import drive\n",
    "# # 將自己的雲端硬碟掛載上去\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# os.chdir('./gdrive/MyDrive/Colab Notebooks/cheng_ta/final/dataset')      # 檔案目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10103ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append('./GWFM/')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from methods.DataIO import StructuralDataSampler, StructuralDataSampler2\n",
    "import baselineModels as bm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b5ad4",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b769e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./dataset/datas.pickle', 'rb') as f:\n",
    "    datas = pickle.load(f)\n",
    "\n",
    "# node labels\n",
    "Nclasses = []\n",
    "for i in range(len(datas)):\n",
    "    temp = torch.tensor(datas[i]['nodesLabel'], dtype=torch.long)\n",
    "    Nclasses.append(temp)\n",
    "\n",
    "classWeight = [0] * 38\n",
    "for i in range(len(Nclasses)):\n",
    "    for j in range(Nclasses[i].shape[0]):\n",
    "        c = Nclasses[i][j]\n",
    "        classWeight[c] += 1\n",
    "classWeight = torch.tensor(classWeight, dtype=torch.float)\n",
    "classWeight = classWeight / torch.sum(classWeight)\n",
    "classWeight = 1 / classWeight\n",
    "classWeight = classWeight / torch.sum(classWeight)\n",
    "del Nclasses\n",
    "classWeight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f612a8",
   "metadata": {},
   "source": [
    "# model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf60465",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSave = './models/baseline/GCNnodeModel.pt'\n",
    "lossSave = './models/baseline/nodeLoss_baseline.pickle'\n",
    "# 訓練超參數 model\n",
    "data_sampler = StructuralDataSampler2(datas)\n",
    "num_samples = len(data_sampler)\n",
    "inputD = 4\n",
    "outputD = 38\n",
    "num_classes = None       # 先驗分布\n",
    "prior = None             # 先驗分布\n",
    "\n",
    "# 訓練超參數 train\n",
    "size_batch = 16\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "weight_decay = 0\n",
    "shuffle_data = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc33736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(bm)\n",
    "model = bm.GCN(inputD, 0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a45184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model,\n",
    "                  data_sampler,\n",
    "                  size_batch: int = 16,\n",
    "                  epochs: int = 10,\n",
    "                  lr: float = 1e-1,\n",
    "                  weight_decay: float = 0,\n",
    "                  shuffle_data: bool = True,\n",
    "                  device=None):\n",
    "    \"\"\"\n",
    "    training a FGWF model\n",
    "    Args:\n",
    "        model: a FGWF model\n",
    "        database: a list of data, each element is a list representing [cost, distriubtion, feature, label]\n",
    "        size_batch: the size of batch, deciding the frequency of backpropagation\n",
    "        epochs: the number epochs\n",
    "        lr: learning rate\n",
    "        weight_decay: the weight of the l2-norm regularization of parameters\n",
    "        shuffle_data: whether shuffle data in each epoch\n",
    "        zeta: the weight of the regularizer enhancing the diversity of atoms\n",
    "        mode: fit or transform\n",
    "        visualize_prefix: display learning result after each epoch or not\n",
    "    \"\"\"\n",
    "    global modelSave, lossSave, classWeight\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  \n",
    "    lossfun = nn.CrossEntropyLoss(weight=classWeight).to(device)\n",
    "    model.train()\n",
    "\n",
    "    num_samples = data_sampler.__len__()\n",
    "    index_samples = list(range(num_samples))\n",
    "    index_split = int(num_samples * 0.8)\n",
    "    \n",
    "    random.shuffle(index_samples)\n",
    "    index_train = index_samples[:index_split]\n",
    "    index_val = index_samples[index_split:]\n",
    "    \n",
    "    epoch_metric = {'loss':[], 'accu':[], 'f1':[]}\n",
    "    \n",
    "    t_start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "\n",
    "        if shuffle_data:\n",
    "            random.shuffle(index_train)\n",
    "        \n",
    "        total_loss = 0.\n",
    "        counts = 0\n",
    "        for i in index_train:\n",
    "            data = data_sampler[i]\n",
    "            adj, features, labels = data[0].to(device), data[2].to(device), data[4].to(device)\n",
    "            y = model(features, adj)\n",
    "            loss = lossfun(y, labels)\n",
    "            total_loss += loss\n",
    "            \n",
    "            counts += 1\n",
    "            if counts % size_batch == 0 or counts == num_samples:\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(total_loss.item())\n",
    "                total_loss = 0.\n",
    "                counts = 0\n",
    "            \n",
    "        el = sum(batch_loss) / len(batch_loss)\n",
    "        epoch_metric['loss'].append(el)\n",
    "        \n",
    "        preds = []\n",
    "        trues = []\n",
    "        for i in index_val:\n",
    "            with torch.no_grad():\n",
    "                data = data_sampler[i]\n",
    "                adj, features, labels = data[0].to(device), data[2].to(device), data[4].to(device)\n",
    "                y = model(features, adj)\n",
    "            pred = torch.argmax(y, dim=1).cpu().detach()\n",
    "            true = labels.cpu().detach()\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        trues = torch.cat(trues, dim=0)\n",
    "            \n",
    "        accu = accuracy_score(preds, trues)\n",
    "        f1 = f1_score(preds, trues, average='macro')\n",
    "        epoch_metric['accu'].append(accu)\n",
    "        epoch_metric['f1'].append(f1)\n",
    "        print('epoch loss: {}, accu/f1:{}/{}, epoch:{}/{}, time:{}'.format(el, accu, f1, epoch, epochs, time.time()-t_start))\n",
    "        \n",
    "        with open(lossSave, 'wb') as f:\n",
    "              pickle.dump(epoch_metric, f)\n",
    "        torch.save(model.state_dict(), modelSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a76d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_baseline(model=model,\n",
    "#               data_sampler=data_sampler,\n",
    "#               size_batch = size_batch,\n",
    "#               epochs = epochs,\n",
    "#               lr = lr,\n",
    "#               weight_decay = weight_decay,\n",
    "#               shuffle_data = shuffle_data,\n",
    "#               device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6777c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/baseline/nodeLoss_baseline.pickle', 'rb') as f:\n",
    "    nodeLoss_baseline = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62fffeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6007776049766719, 0.06697287885806016)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nodeLoss_baseline['accu']), max(nodeLoss_baseline['f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
