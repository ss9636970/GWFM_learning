{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import Google Drive 套件\n",
    "from google.colab import drive\n",
    "# 將自己的雲端硬碟掛載上去\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "os.chdir('./gdrive/MyDrive/Colab Notebooks/cheng_ta/final/dataset')      # 檔案目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f401808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import importlib\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import predictModels as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cc317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/datas.pickle', 'rb') as f:\n",
    "    datas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4d8c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], torch.Size([2000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph labels\n",
    "classes = []\n",
    "for i in range(len(datas)):\n",
    "    if datas[i]['graphLabel'] not in classes:\n",
    "        classes.append(datas[i]['graphLabel'])\n",
    "        \n",
    "Gclasses = []\n",
    "for i in range(len(datas)):\n",
    "    Gclasses.append(datas[i]['graphLabel'])\n",
    "Gclasses = torch.tensor(Gclasses, dtype=torch.long)\n",
    "classes, Gclasses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739a59e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([31385]), tensor(37))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node labels\n",
    "Nclasses = []\n",
    "for i in range(len(datas)):\n",
    "    Nclasses.append(datas[i]['nodesLabel'])\n",
    "Nclasses = np.concatenate(Nclasses, axis=0)\n",
    "Nclasses = torch.tensor(Nclasses, dtype=torch.long)\n",
    "\n",
    "classWeight = [0] * 38\n",
    "for i in range(Nclasses.shape[0]):\n",
    "    c = Nclasses[i]\n",
    "    classWeight[c] += 1\n",
    "classWeight = torch.tensor(classWeight, dtype=torch.float)\n",
    "classWeight = classWeight / torch.sum(classWeight)\n",
    "classWeight = 1 / classWeight\n",
    "classWeight = classWeight / torch.sum(classWeight)\n",
    "\n",
    "Nclasses.shape, torch.max(Nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbe9cc",
   "metadata": {},
   "source": [
    "# graph 1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bca72f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 36])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Attributes/graphAttributes_1.pickle', 'rb') as f:\n",
    "    graphAttributes_1 = pickle.load(f)\n",
    "graphAttributes_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a41c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 參數\n",
    "inputD = 36\n",
    "hD = 18\n",
    "outputD = 2\n",
    "\n",
    "# 訓練超參數 train\n",
    "modelSave = './models/pre1/graphModel_1.pt'\n",
    "lossSave = './models/pre1/graphLoss_1.pickle'\n",
    "size_batch = 16\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "shuffle_data = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bac185af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.predModel_1(inputD, hD, outputD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0663a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graphAttributes(model,\n",
    "                          graphAttributes_1,\n",
    "                          labels,\n",
    "                          size_batch: int = 500,\n",
    "                          epochs: int = 100,\n",
    "                          lr: float = 1e-1,\n",
    "                          weight_decay: float = 0,\n",
    "                          shuffle_data: bool = True,\n",
    "                          device=None):\n",
    "    \"\"\"\n",
    "    training a FGWF model\n",
    "    Args:\n",
    "        model: a FGWF model\n",
    "        database: a list of data, each element is a list representing [cost, distriubtion, feature, label]\n",
    "        size_batch: the size of batch, deciding the frequency of backpropagation\n",
    "        epochs: the number epochs\n",
    "        lr: learning rate\n",
    "        weight_decay: the weight of the l2-norm regularization of parameters\n",
    "        shuffle_data: whether shuffle data in each epoch\n",
    "        zeta: the weight of the regularizer enhancing the diversity of atoms\n",
    "        mode: fit or transform\n",
    "        visualize_prefix: display learning result after each epoch or not\n",
    "    \"\"\"\n",
    "    global modelSave, lossSave\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if device is not None:\n",
    "        print(device)\n",
    "        model.to(device)\n",
    "        graphAttributes_1 = graphAttributes_1.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    num_samples = graphAttributes_1.shape[0]\n",
    "    index_samples = list(range(num_samples))\n",
    "    index_split = int(num_samples * 0.8)\n",
    "    \n",
    "    random.shuffle(index_samples)\n",
    "    index_train = index_samples[:index_split]\n",
    "    index_val = index_samples[index_split:]\n",
    "    \n",
    "    loops = int(np.ceil(len(index_train) / size_batch))\n",
    "    epoch_metric = {'loss':[], 'val':[]}\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if shuffle_data:\n",
    "            random.shuffle(index_train)\n",
    "        \n",
    "        for loop in range(loops):\n",
    "            indexes = index_train[loop * size_batch:loop * size_batch + size_batch]\n",
    "            x = graphAttributes_1[indexes, :]\n",
    "            y = model(x)\n",
    "            l = labels[indexes]\n",
    "            loss = lossfun(y, l)\n",
    "            batch_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        el = sum(batch_loss) / len(batch_loss)\n",
    "        epoch_metric['loss'].append(el)\n",
    "        with torch.no_grad():\n",
    "            x = graphAttributes_1[index_val, :]\n",
    "            y = model(x)\n",
    "            pred = torch.argmax(y, dim=1).cpu().detach()\n",
    "            true = labels[index_val].cpu().detach()\n",
    "            accu = accuracy_score(pred, true)\n",
    "        epoch_metric['val'].append(accu)\n",
    "        print('epoch loss: {},   val_accu:{},   epoch:{}/{},   time:{}'.format(el, accu, epoch, epochs, time.time()-t_start))\n",
    "        \n",
    "        with open(lossSave, 'wb') as f:\n",
    "              pickle.dump(epoch_metric, f)\n",
    "        torch.save(model.state_dict(), modelSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "247fb80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch loss: 0.3565731942653656,   val_accu:0.97,   epoch:0/100,   time:0.007979393005371094\n"
     ]
    }
   ],
   "source": [
    "train_graphAttributes(model=model,\n",
    "                      graphAttributes_1=graphAttributes_1,\n",
    "                      labels=Gclasses,\n",
    "                      size_batch=size_batch,\n",
    "                      epochs=epochs,\n",
    "                      lr=lr,\n",
    "                      weight_decay=weight_decay,\n",
    "                      shuffle_data=shuffle_data,\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fb484c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/pre1/graphLoss_1.pickle', 'rb') as f:\n",
    "    graphLoss_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "08d7df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3565731942653656], 'val': [0.97]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphLoss_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911631c8",
   "metadata": {},
   "source": [
    "# node 1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dab02457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31385, 5040])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Attributes/nodeAttributes_1.pickle', 'rb') as f:\n",
    "    nodeAttributes_1 = pickle.load(f)\n",
    "nodeAttributes_1 = torch.cat(nodeAttributes_1, dim=0)\n",
    "nodeAttributes_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad22e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 參數\n",
    "inputD = 5040\n",
    "hD = 2500\n",
    "outputD = 38\n",
    "\n",
    "# 訓練超參數 train\n",
    "modelSave = './models/pre1/nodeModel_1.pt'\n",
    "lossSave = './models/pre1/nodeLoss_1.pickle'\n",
    "size_batch = 500\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "shuffle_data = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd86e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.predModel_1(inputD, hD, outputD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nodeAttributes(model,\n",
    "                        nodeAttributes_1,\n",
    "                        labels,\n",
    "                        size_batch: int = 500,\n",
    "                        epochs: int = 100,\n",
    "                        lr: float = 1e-1,\n",
    "                        weight_decay: float = 0,\n",
    "                        shuffle_data: bool = True,\n",
    "                        device=None):\n",
    "    \"\"\"\n",
    "    training a FGWF model\n",
    "    Args:\n",
    "        model: a FGWF model\n",
    "        database: a list of data, each element is a list representing [cost, distriubtion, feature, label]\n",
    "        size_batch: the size of batch, deciding the frequency of backpropagation\n",
    "        epochs: the number epochs\n",
    "        lr: learning rate\n",
    "        weight_decay: the weight of the l2-norm regularization of parameters\n",
    "        shuffle_data: whether shuffle data in each epoch\n",
    "        zeta: the weight of the regularizer enhancing the diversity of atoms\n",
    "        mode: fit or transform\n",
    "        visualize_prefix: display learning result after each epoch or not\n",
    "    \"\"\"\n",
    "    global modelSave, lossSave, classWeight\n",
    "    \n",
    "    if device is not None:\n",
    "        print(device)\n",
    "        model.to(device)\n",
    "        nodeAttributes_1 = nodeAttributes_1.to(device)\n",
    "        labels = labels.to(device)\n",
    "        classWeight = classWeight.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lossfun = nn.CrossEntropyLoss(weight=classWeight)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    num_samples = nodeAttributes_1.shape[0]\n",
    "    index_samples = list(range(num_samples))\n",
    "    index_split = int(num_samples * 0.8)\n",
    "    \n",
    "    random.shuffle(index_samples)\n",
    "    index_train = index_samples[:index_split]\n",
    "    index_val = index_samples[index_split:]\n",
    "    \n",
    "    loops = int(np.ceil(len(index_train) / size_batch))\n",
    "    epoch_metric = {'loss':[], 'accu':[], 'f1':[]}\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if shuffle_data:\n",
    "            random.shuffle(index_train)\n",
    "        \n",
    "        for loop in range(loops):\n",
    "            indexes = index_train[loop * size_batch:loop * size_batch + size_batch]\n",
    "            x = nodeAttributes_1[indexes, :]\n",
    "            y = model(x)\n",
    "            l = labels[indexes]\n",
    "            loss = lossfun(y, l)\n",
    "            batch_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        el = sum(batch_loss) / len(batch_loss)\n",
    "        epoch_metric['loss'].append(el)\n",
    "        with torch.no_grad():\n",
    "            x = nodeAttributes_1[index_val, :]\n",
    "            y = model(x)\n",
    "            pred = torch.argmax(y, dim=1).cpu().detach()\n",
    "            true = labels[index_val].cpu().detach()\n",
    "            accu = accuracy_score(pred, true)\n",
    "            f1 = f1_score(pred, true, average='macro')\n",
    "        epoch_metric['accu'].append(accu)\n",
    "        epoch_metric['f1'].append(f1)\n",
    "        print('epoch loss: {}, accu/f1:{}/{}, epoch:{}/{}, time:{}'.format(el, accu, f1, epoch, epochs, time.time()-t_start))\n",
    "        \n",
    "        with open(lossSave, 'wb') as f:\n",
    "              pickle.dump(epoch_metric, f)\n",
    "        torch.save(model.state_dict(), modelSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc12102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodeAttributes(model=model,\n",
    "                      nodeAttributes_1=nodeAttributes_1,\n",
    "                      labels=Nclasses,\n",
    "                      size_batch=size_batch,\n",
    "                      epochs=epochs,\n",
    "                      lr=lr,\n",
    "                      weight_decay=weight_decay,\n",
    "                      shuffle_data=shuffle_data,\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/pre1/nodeLoss_1.pickle', 'rb') as f:\n",
    "    graphLoss_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a871b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
